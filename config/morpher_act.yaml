# Morpher Training Config (ACT enabled)
#
# Uses Morpher with Adaptive Computation Time (ACT)

defaults:
  - _self_

hydra:
  output_subdir: null

# Model architecture
arch:
  name: MorpherACT
  loss:
    name: ACTLossHead
    loss_type: stablemax_cross_entropy
  
  # ACT settings  
  halt_exploration_prob: 0.1
  halt_max_steps: 16
  
  # Morpher architecture
  d: 64                      # per-slot dimension
  time_scales: [1, 2, 4]     # multi-scale slots
  enc_dec_rank: 32           # LoRA rank for encoder/decoder
  mixer_expansion: 4.0
  dropout: 0.0
  
  # Stacking
  num_levels: 1
  cycles_per_level: [2]      # cycles per ACT step
  
  # I/O
  io_dim: 512
  puzzle_emb_ndim: 512
  
  # Attention
  stream_head_assignment: shared
  head_input_scope: slot
  attn_backend: sdpa
  pos_encodings: rope

# Data path
data_path: data/arc-aug-1000

# Hyperparams - Training
global_batch_size: 768

epochs: 100000
eval_interval: 10000
checkpoint_every_eval: True

lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 2000

# Standard hyperparameter settings for LM, as used in Llama
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
puzzle_emb_weight_decay: 0.1

# Hyperparams - Puzzle embeddings training
puzzle_emb_lr: 1e-2
